# Test suite for Forward Learned and Markovian discrete diffusion implementation generated by Gemini 
# WARNING: little human verification applied, tests may contain errors.

import torch
import torch.nn.functional as F
import unittest
from src.discrete_diffusion import FLDD, Uniform, Masking, DiscreteDiffusion, DummyX0Model

class MockNet(torch.nn.Module):
    """
    A minimal forward network for testing purposes.
    Unlike a dummy linear layer, this network explicitly uses both 't' and 'x'
    to ensure we can verify gradient flows and conditional dependencies.
    """
    def __init__(self, num_classes, embedding_dim=16):
        super().__init__()
        self.num_classes = num_classes
        # Time embedding
        self.time_mlp = torch.nn.Sequential(
            torch.nn.Linear(1, embedding_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embedding_dim, embedding_dim)
        )
        # Data embedding (handles both indices and soft probabilities)
        self.x_proj = torch.nn.Linear(num_classes, embedding_dim)
        # Output head
        self.head = torch.nn.Linear(embedding_dim, num_classes)

    def forward(self, x, t, cond=None):
        # t: (B,) -> (B, 1)
        t_emb = self.time_mlp(t.float().view(-1, 1) / 100.0)
        
        # x: Can be (B, ...) indices OR (B, ..., K) soft probs
        if x.dtype == torch.long or x.dtype == torch.int:
            x_feat = F.one_hot(x.long(), num_classes=self.num_classes).float()
        else:
            x_feat = x
            
        # Project x to embedding dimension
        x_emb = self.x_proj(x_feat)
        
        # Combine (Broadcast t across spatial dims of x)
        # If x is (B, H, W, K), x_emb is (B, H, W, D)
        # t_emb is (B, D) -> (B, 1, 1, D)
        while t_emb.dim() < x_emb.dim():
            t_emb = t_emb.unsqueeze(1)
            
        out = self.head(x_emb + t_emb)
        return out

class TestFLDDImplementation(unittest.TestCase):
    def setUp(self):
        self.K = 5   # Number of classes
        self.T = 10  # Number of timesteps
        self.B = 4   # Batch size
        self.D = 8   # Sequence length / Spatial dim
        
        # Initialize FLDD with the MockNet
        self.f_net = MockNet(self.K)
        self.fldd = FLDD(num_classes=self.K, num_timesteps=self.T, forward_net=self.f_net)

    # ------------------------------------------------------------------------
    # Core Mathematical Properties (Paper Section 3.2)
    # ------------------------------------------------------------------------

    def test_coupling_stochasticity(self):
        """
        Verifies that the Maximum Coupling transition matrix Q is a valid 
        probability distribution (rows sum to 1).
        """
        u_t = torch.softmax(torch.randn(self.B, self.K), dim=-1)
        u_s = torch.softmax(torch.randn(self.B, self.K), dim=-1)
        
        # Q shape: (B, K_from, K_to)
        Q = self.fldd._compute_coupling(u_s, u_t)
        
        row_sums = Q.sum(dim=-1)
        self.assertTrue(
            torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5), 
            f"Transition matrix rows must sum to 1. Got mean sum: {row_sums.mean()}"
        )

    def test_identity_coupling(self):
        """
        Verifies that if the marginals are identical (u_s == u_t), 
        the coupling mechanism produces an Identity matrix (minimized transport).
        """
        u = torch.softmax(torch.randn(1, self.K), dim=-1)
        Q = self.fldd._compute_coupling(u, u)
        
        I = torch.eye(self.K)
        self.assertTrue(
            torch.allclose(Q.squeeze(), I, atol=1e-5), 
            "Coupling should be Identity when source and target distributions are identical."
        )

    def test_maximum_coupling_minimality(self):
        """
        Verifies the "Maximum Coupling" property (Eq 11).
        Probability of staying should be min(u_s, u_t) / u_t.
        """
        # Setup uniform distribution [0.2, 0.2, 0.2, 0.2, 0.2]
        u_t = torch.full((1, self.K), 1.0/self.K) 
        u_s = u_t.clone()
        
        # Create imbalance: Index 0 has surplus, Index 1 has deficit
        u_s[0, 0] += 0.1 # 0.3
        u_s[0, 1] -= 0.1 # 0.1
        
        Q = self.fldd._compute_coupling(u_s, u_t)
        diag = torch.diagonal(Q[0], dim1=0, dim2=1)
        
        # Case 1: Surplus (u_s > u_t). 
        # We keep all existing mass. P(stay) = 0.2 / 0.2 = 1.0
        self.assertAlmostEqual(diag[0].item(), 1.0, places=5)
        
        # Case 2: Deficit (u_s < u_t).
        # We keep only u_s mass. P(stay) = 0.1 / 0.2 = 0.5
        self.assertAlmostEqual(diag[1].item(), 0.5, places=5)

    def test_marginal_consistency(self):
        """
        [CRITICAL] Verifies Eq 10: q(z_s|x) = sum_{z_t} q(z_t|x) * q(z_s|z_t, x).
        This ensures the posterior correctly transports mass between the learned marginals.
        """
        x_0 = torch.randint(0, self.K, (self.B, self.D))
        t_val = 5
        t = torch.full((self.B,), t_val)
        
        # Get learned marginals from the network
        u_t = torch.softmax(self.fldd._get_marginals(x_0, t), dim=-1)     # q(z_t|x)
        u_s = torch.softmax(self.fldd._get_marginals(x_0, t-1), dim=-1)   # q(z_s|x)
        
        # Compute transition matrix Q
        Q = self.fldd._compute_coupling(u_s, u_t) 
        
        # Apply transition: u_new = u_t @ Q
        calculated_u_s = torch.einsum("...k,...kj->...j", u_t, Q)
        
        self.assertTrue(
            torch.allclose(calculated_u_s, u_s, atol=1e-5), 
            "Posterior distribution does not strictly satisfy marginal consistency (Eq 10)."
        )

    # ------------------------------------------------------------------------
    # Training Dynamics (Warm-up & REINFORCE) - Paper Section 3.3
    # ------------------------------------------------------------------------

    def test_warmup_differentiability(self):
        """
        Verifies Section 3.3 "Relaxed Warm-Up".
        During the warm-up phase, the model should return Soft Tensors (Gumbel-Softmax)
        and gradients should flow back to the forward network via reparameterization.
        """
        # Force Warmup Mode
        self.fldd.warmup_steps = 1000
        self.fldd.step_schedule(global_step=10) # step < warmup_steps
        self.assertFalse(self.fldd.use_reinforce, "Model should be in Warmup mode (use_reinforce=False)")
        
        x_0 = torch.randint(0, self.K, (self.B, self.D))
        t = torch.randint(1, self.T, (self.B,))
        
        # Sample z_t
        z_t_soft, _ = self.fldd.sample(x_0, t, noise=None)
        
        # Check output type
        self.assertTrue(z_t_soft.is_floating_point(), "Warmup sampling must return float tensors (soft probabilities).")
        self.assertEqual(z_t_soft.shape[-1], self.K, "Warmup samples must have dimension K (last dim).")
        
        # Check Gradient Flow
        loss = z_t_soft.sum()
        loss.backward()
        
        has_grad = any(p.grad is not None for p in self.f_net.parameters())
        self.assertTrue(has_grad, "Gradients failed to flow through forward_net during warmup phase.")

    def test_reinforce_gradient_presence(self):
        """
        Verifies Section 3.3 "Unbiased Optimization".
        During the REINFORCE phase, the `get_auxiliary_loss` function must generate
        gradients for the forward network using the REINFORCE estimator.
        """
        # Force REINFORCE Mode
        self.fldd.warmup_steps = 0
        self.fldd.step_schedule(global_step=100)
        self.assertTrue(self.fldd.use_reinforce, "Model should be in REINFORCE mode")

        x_0 = torch.randint(0, self.K, (self.B, self.D))
        t = torch.randint(1, self.T, (self.B,))
        
        # 1. Sample (Discrete)
        noise = torch.rand(self.B, self.D, self.K)
        z_t, log_probs = self.fldd.sample(x_0, t, noise)
        
        self.assertFalse(z_t.is_floating_point(), "REINFORCE sampling must return integer indices.")
        
        # 2. Compute Loss
        # Create a dummy KL divergence signal
        kl_div = torch.randn(self.B * self.D) 
        
        loss = self.fldd.get_auxiliary_loss(x_0, z_t, t, kl_div=kl_div, log_probs=log_probs)
        
        # 3. Check Gradients
        if isinstance(loss, torch.Tensor):
            self.f_net.zero_grad()
            loss.backward()
            has_grad = any(p.grad is not None for p in self.f_net.parameters())
            self.assertTrue(has_grad, "REINFORCE loss did not provide gradients to forward_net.")
        else:
            self.fail("get_auxiliary_loss returned 0.0 or None in REINFORCE mode.")

    # ------------------------------------------------------------------------
    # Edge Cases & Shapes
    # ------------------------------------------------------------------------

    def test_boundary_limit_t0(self):
        """
        Verifies boundary condition: q(z_0 | x) should be a Delta function on x.
        """
        x_0 = torch.randint(0, self.K, (self.B, self.D))
        t = torch.zeros(self.B).long() # t=0
        
        logits = self.fldd._get_marginals(x_0, t)
        probs = torch.softmax(logits, dim=-1)
        
        # Target: One-hot encoding of x_0
        target = F.one_hot(x_0, num_classes=self.K).float()
        
        # We allow a small tolerance because of the log-linear interpolation control_ratio
        self.assertTrue(
            torch.allclose(probs, target, atol=1e-2), 
            "Marginal at t=0 must approximate a delta distribution on x_0."
        )

    def test_spatial_broadcasting(self):
        """
        Verifies that the FLDD logic handles Image Data (B, H, W).
        Crucial because _compute_coupling uses einsum which requires careful dimension handling.
        """
        H, W = 16, 16
        x_0 = torch.randint(0, self.K, (self.B, H, W)) # (B, H, W) indices
        t = torch.randint(1, self.T, (self.B,))
        
        # x_t for posterior calculation (Discrete/REINFORCE case)
        x_t = torch.randint(0, self.K, (self.B, H, W))
        
        # Should run without shape mismatch errors
        post_logits = self.fldd.compute_posterior_logits(x_0, x_t, t)
        
        # Expected shape: (B, H, W, K)
        self.assertEqual(post_logits.shape, (self.B, H, W, self.K))
        
        # Check valid probability distribution
        probs = torch.softmax(post_logits, dim=-1)
        sums = probs.sum(dim=-1)
        self.assertTrue(
            torch.allclose(sums, torch.ones(self.B, H, W), atol=1e-5),
            "Posterior probabilities across spatial dimensions do not sum to 1."
        )

    def test_posterior_with_predicted_logits(self):
        """
        Verifies that `compute_posterior_logits` handles soft inputs (logits).
        During validation/generation, x_0 comes from the denoising model (logits),
        not ground truth (indices).
        """
        # Simulated model output (Logits from Denoising Net)
        pred_x0_logits = torch.randn(self.B, self.D, self.K)
        x_t = torch.randint(0, self.K, (self.B, self.D))
        t = torch.full((self.B,), 5)
        
        posterior = self.fldd.compute_posterior_logits(pred_x0_logits, x_t, t)
        
        self.assertEqual(posterior.shape, (self.B, self.D, self.K))
        self.assertFalse(torch.isnan(posterior).any(), "Posterior computation with soft logits produced NaNs.")


class TestMarkovDiffusion(unittest.TestCase):
    def setUp(self):
        self.K = 4   # Classes
        self.T = 10  # Timesteps
        self.B = 2   # Batch
        self.D = 8   # Dim

    def test_uniform_matrix_stochasticity(self):
        """Test that all transition matrices in Uniform sums to 1."""
        uni = Uniform(self.K, self.T)
        
        # Check One-step matrices (t-1 -> t)
        # Note: We need to access the buffer appropriately
        q_one = uni.q_one_step_transposed.transpose(1, 2)
        rows = q_one.sum(dim=-1)
        self.assertTrue(torch.allclose(rows, torch.ones_like(rows)), "One-step matrices must be row-stochastic")

        # Check Cumulative matrices (0 -> t)
        q_cumulative = uni.q_mats
        rows_cum = q_cumulative.sum(dim=-1)
        self.assertTrue(torch.allclose(rows_cum, torch.ones_like(rows_cum)), "Cumulative matrices must be row-stochastic")

    def test_uniform_stationary_distribution(self):
        """Test that Uniform process converges to uniform distribution at T."""
        uni = Uniform(self.K, self.T)
        final_mat = uni.q_mats[-1] # Matrix at T
        
        # Each row should be roughly [0.25, 0.25, 0.25, 0.25]
        expected = torch.ones_like(final_mat) / self.K
        
        # We allow a relaxed tolerance because beta schedule might not hit T exactly depending on params
        self.assertTrue(torch.allclose(final_mat, expected, atol=0.05), 
                        "Process did not converge to uniform distribution.")

    def test_masking_absorbing_state(self):
        """Test that the Mask token stays Masked."""
        masking = Masking(self.K, self.T)
        
        # The mask index is K (since 0..K-1 are data)
        mask_idx = self.K
        
        # Get one-step matrix (t-1 -> t)
        # q_one_step_transposed is (T, K+1, K+1)
        # We want to check that P(x_t = MASK | x_{t-1} = MASK) == 1.0
        
        q_transposed = masking.q_one_step_transposed
        
        # q_transposed[t, i, j] implies P(x_{t-1}=j | x_t=i) ?? 
        # Let's check the code: 
        # q_onestep_mats was created as [Row, Col] -> P(Col|Row).
        # q_one_step_transposed is Transpose(1, 2) -> P(Row|Col).
        
        # Let's look at the implementation of Masking again:
        # mat[self.mask_token_idx, self.mask_token_idx] = 1.0
        # This is the diagonal.
        
        # We verify logical correctness of the transition logic directly
        # If input is MASK, output must be MASK
        x_0 = torch.full((self.B, self.D), mask_idx).long()
        t = torch.randint(1, self.T, (self.B,))
        noise = torch.rand(self.B, self.D, self.K + 1)
        
        x_t, _ = masking.sample(x_0, t, noise)
        
        self.assertTrue(torch.all(x_t == mask_idx), "Mask token must be absorbing (cannot change).")

    def test_posterior_probability_validity(self):
        """Test that computed posteriors sum to 1."""
        uni = Uniform(self.K, self.T)
        
        x_0 = torch.randint(0, self.K, (self.B, self.D))
        x_t = torch.randint(0, self.K, (self.B, self.D))
        t = torch.randint(2, self.T, (self.B,)) # t > 1
        
        logits = uni.compute_posterior_logits(x_0, x_t, t)
        probs = torch.softmax(logits, dim=-1)
        
        sums = probs.sum(dim=-1)
        self.assertTrue(torch.allclose(sums, torch.ones_like(sums)), 
                        "Posterior probabilities must sum to 1.")

    def test_vb_loss_non_negativity(self):
        """Test that KL divergence in training is non-negative."""
        # Use Dummy Model
        x0_model = DummyX0Model(n_channel=1, N=self.K, num_timesteps=self.T)
        diffusion = DiscreteDiffusion(x0_model, self.T, self.K, forward_process=Uniform(self.K, self.T))
        
        x = torch.randint(0, self.K, (2, 1, 32, 32)) # Image data
        cond = torch.randint(0, 10, (2,))
        
        # Check loss
        loss, info = diffusion(x, cond)
        
        self.assertGreaterEqual(info['vb_loss'], -1e-5, "VB Loss (KL) cannot be negative.")


if __name__ == "__main__":
    unittest.main()